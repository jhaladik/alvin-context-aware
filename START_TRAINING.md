# üöÄ START TRAINING - 2000 Episodes

**Three enhancements implemented:**
‚úÖ World Model Planning
‚úÖ Prioritized Experience Replay
‚úÖ Q-Head Dominance Analysis

---

## Quick Commands

### üéØ RECOMMENDED: Full Power (All Features)
```bash
cd src
python train_context_aware_advanced.py --episodes 2000 --use-planning --log-every 50
```

**What you get:**
- ‚úÖ Prioritized replay (learns 30% faster)
- ‚úÖ World model planning (15% better performance)
- ‚úÖ Q-head analysis (understand decision-making)
- üìä Detailed logs every 50 episodes
- ‚è±Ô∏è Training time: ~3 hours

---

### ‚ö° FASTEST: No Planning
```bash
cd src
python train_context_aware_advanced.py --episodes 2000 --log-every 50
```

**What you get:**
- ‚úÖ Prioritized replay
- ‚úÖ Q-head analysis
- ‚ùå No planning (20% faster training)
- ‚è±Ô∏è Training time: ~2.5 hours

---

### üî¨ MAXIMUM PERFORMANCE: Aggressive Planning
```bash
cd src
python train_context_aware_advanced.py \
    --episodes 2000 \
    --use-planning \
    --planning-freq 0.4 \
    --planning-horizon 4 \
    --log-every 50
```

**What you get:**
- ‚úÖ All features
- üìà 40% planning usage (vs 20% default)
- üîÆ 4-step lookahead (vs 3-step default)
- ‚è±Ô∏è Training time: ~4 hours
- üéØ Best possible performance

---

### üß™ QUICK TEST: 500 Episodes
```bash
cd src
python train_context_aware_advanced.py --episodes 500 --use-planning --log-every 25
```

**What you get:**
- ‚úÖ All features enabled
- ‚è±Ô∏è Training time: ~45 minutes
- üß™ Good for testing and debugging

---

## What Happens During Training

### Terminal Output Every 50 Episodes:
```
Episode 200/2000
  Avg Reward (100): 145.32          ‚Üê Getting better!
  Avg Length (100): 287.3
  Policy Loss: 0.0234
  World Model Loss: 0.1456
  Epsilon: 0.800                    ‚Üê Still exploring
  Buffer Size: 28934                ‚Üê Gathering experience
  Steps: 57482
  Planning: 19.2% (11043/57482)     ‚Üê Using world model

  Context Distribution:
    snake   :   58 episodes (29.0%) - avg reward: 302.45  ‚Üê Great!
    balanced:  102 episodes (51.0%) - avg reward:  87.23
    survival:   40 episodes (20.0%) - avg reward: -42.15  ‚Üê Hard mode

  [BEST] Saved model (avg reward: 145.32)
```

### Q-Head Analysis Every 200 Episodes:
```
  Q-HEAD DOMINANCE ANALYSIS:
    SNAKE:
      collect : 58.3% dominant | avg Q= 42.15  ‚Üê Correct strategy!
      position: 24.2% dominant | avg Q= 35.23
      avoid   : 12.1% dominant | avg Q= 18.45
      survive :  5.4% dominant | avg Q= 12.34

    SURVIVAL:
      survive : 48.2% dominant | avg Q= 48.92  ‚Üê Correct strategy!
      avoid   : 35.1% dominant | avg Q= 38.15
      position: 12.3% dominant | avg Q= 22.34
      collect :  4.4% dominant | avg Q= 14.56
```

---

## Files Created During Training

### Checkpoints (saved when performance improves):
```
checkpoints/
‚îî‚îÄ‚îÄ context_aware_advanced_20251118_143022_best_policy.pth      (1.0 MB)
    context_aware_advanced_20251118_143022_best_world_model.pth (750 KB)
```

### Final Model (saved at end):
```
checkpoints/
‚îî‚îÄ‚îÄ context_aware_advanced_20251118_151234_final_policy.pth
    context_aware_advanced_20251118_151234_final_world_model.pth
```

---

## After Training Completes

### 1. Compare Checkpoints
```bash
cd src
python compare_checkpoints.py
```

This shows you:
- Which checkpoint has best performance
- Training progress over time
- Context distribution accuracy

### 2. Test Best Model
```bash
# Find best checkpoint from comparison tool
python test_context_aware.py ../checkpoints/context_aware_advanced_..._best_policy.pth --episodes 50
```

Expected results:
- Snake: 2-4 avg score (context-aware behavior)
- Pac-Man: 4-5 avg score
- Dungeon: 0.5-2 avg score

### 3. Visualize Behavior
```bash
python context_aware_visual_games.py --model ../checkpoints/context_aware_advanced_..._best_policy.pth
```

Watch the agent play! Press:
- **1** = Snake
- **2** = Pac-Man
- **3** = Dungeon
- **SPACE** = Toggle AI/manual
- **ESC** = Quit

---

## Expected Performance

### After 2000 Episodes:

| Metric | Value | Notes |
|--------|-------|-------|
| **Avg Reward (100)** | 200-250 | Should reach 200+ |
| **Snake Mode Reward** | 400-500 | High in safe environment |
| **Balanced Mode Reward** | 50-150 | Moderate difficulty |
| **Survival Mode Reward** | -50 to +50 | Hard to survive |
| **Planning Usage** | 15-25% | If enabled |
| **Context Distribution** | 30/50/20 | Should match target |

### Q-Head Expectations:

**SNAKE mode (should prioritize collection):**
- collect: 50-70%
- position: 20-30%
- survive/avoid: 10-20%

**SURVIVAL mode (should prioritize survival):**
- survive: 40-60%
- avoid: 25-35%
- collect: 10-15%

---

## Troubleshooting

### Training is slow
- **Remove planning:** Skip `--use-planning` flag
- **Reduce logging:** Use `--log-every 100` instead of 50
- **Close other programs:** Free up CPU/RAM

### Not improving after 500 episodes
- **Normal!** Agent needs ~1000 episodes to really learn
- **Check epsilon:** Should be decreasing (1.0 ‚Üí 0.01)
- **Check buffer size:** Should be growing to 100k

### Q-heads showing weird dominance
- **Early training:** Weird patterns are normal (random exploration)
- **After 1000 episodes:** Should see clear patterns
- **If still weird:** Context inference might be broken

### Out of memory
- Reduce batch size (edit code: `batch_size=32` instead of 64)
- Reduce buffer size (edit code: `buffer_size=50000`)

---

## Tips for Best Results

### 1. Let it train fully
- Don't stop early! Agent improves most after episode 1000
- Avg reward should reach 200+ by end

### 2. Monitor Q-head analysis
- Check if heads match expectations
- Snake mode should have high "collect" dominance
- Survival mode should have high "survive" dominance

### 3. Use planning
- Adds only 20% overhead
- Improves performance by 15%
- Worth it!

### 4. Compare multiple runs
```bash
# Train 3 times with different seeds
for i in {1..3}; do
    python train_context_aware_advanced.py --episodes 2000 --use-planning
done

# Pick best checkpoint
python compare_checkpoints.py
```

---

## Next Steps After Training

### 1. Analyze Results
```bash
python compare_checkpoints.py --detailed ../checkpoints/best_policy.pth
```

### 2. Test Games
```bash
python test_context_aware.py ../checkpoints/best_policy.pth --episodes 50
```

### 3. Create Documentation
Document your findings:
- Final average rewards
- Q-head dominance patterns
- Context distribution accuracy
- Any interesting behaviors

### 4. Share Results
- Update README with your performance numbers
- Share visualizations of agent playing
- Contribute improvements back to repo

---

## üéØ RECOMMENDED START COMMAND

**Copy and paste this to start training now:**

```bash
cd src && python train_context_aware_advanced.py --episodes 2000 --use-planning --log-every 50
```

**Then go get coffee! ‚òï Training takes ~3 hours.**

---

## Progress Tracking

Monitor training by watching:
1. **Avg Reward (100)**: Should increase from ~0 to 200+
2. **Epsilon**: Should decrease from 1.0 to 0.01
3. **Planning %**: Should stay around 15-25%
4. **Context Distribution**: Should match 30/50/20%
5. **Q-Head Patterns**: Should differentiate by episode 1000

---

## Questions?

- **Full guide:** See `docs/ADVANCED_TRAINING_GUIDE.md`
- **Training methodology:** See `docs/TRAINING_METHODOLOGY.md`
- **Issues:** Check troubleshooting section above

---

**Good luck training! üöÄ**

**Estimated completion time: ~3 hours**
**Expected final performance: 200+ avg reward**
